---
title: "SocialMediaMining"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval=FALSE}
# install relevant packages

install.packages("knitr",repos = "http://cran.us.r-project.org")
install.packages("twitteR")
install.packages("ROAuth")
install.packages("rtweet")
install.packages("tidytext")
install.packages("plyr")
install.packages("stringr")
install.packages("tm")
install.packages("ggplot2")
install.packages("wordcloud")
install.packages("png")
install.packages("shiny")
install.packages("LDAvis")
install.packages("lda")
install.packages("servr")
install.packages("syuzhet")
install.packages("plotly")
install.packages("tidyverse")
install.packages("openxlsx")
install.packages("tibble")
install.packages("textstem")
install.packages("textmineR")

```


```{r, eval=FALSE}
library(twitteR)
library(ROAuth)

# authenticate with Twitter

consumerKey<-	"*****"
consumerSecret<-"*****"

accessToken<-"******"
accessSecret<-"*****"

setup_twitter_oauth (consumerKey, consumerSecret, accessToken, accessSecret)  # authenticate

```



```{r, eval=FALSE}
library(rtweet)
library(tidyverse)

 create_token(
   consumer_key = consumerKey,
   consumer_secret = consumerSecret
 )

 keyword_list = list("TheLockdown","StayAtHome", "lockdown","social distancing","quarantine","SideEffectsOfQuarantineLife","work from home","workfromhome","StayAtHomeChallenge","QuarantineAndChill","StayAtHomeAndStaySafe","SocialDistancing","StayHomeSaveLives","TogetherAtHome","WashYourHands","self-isolation","isolation")

for (keyword in keyword_list){
   keyword_search = paste(keyword,"-filter:retweets", sep=" ")
   tweets<-search_tweets(keyword_search, n=40000, since=NULL, until=NULL, locale=NULL, geocode="54.141726,-2.2194,350mi", sinceID=NULL, maxID=NULL, resultType=NULL, retryOnRateLimit=120, lang = "en")
   print(keyword)
   filename <- ''
   filename = paste(filename,"./data/", sep='')
   filename = paste(filename,keyword, sep='')
   filename <- gsub(" ", "_", filename)
   filename = paste(filename,Sys.Date(),sep='_')
   filename = paste(filename,'.csv',sep='')
   # put tweets in a csv file
   save_as_csv(tweets,filename, prepend_ids = TRUE, na = "", fileEncoding = "UTF-8")
}



```


```{r,eval=FALSE}
#Code to combine the datas and move the files into a subfolder for each day 

library(openxlsx)
library(filesstrings)


 path <- "./data"
 merge_file_name <- "./data/merged_all"
 merge_file_name <- paste(merge_file_name,Sys.Date(),sep="_")
 merge_file_name <- paste(merge_file_name,".csv",sep="")

 dirpath = "./data/Data_"
 dirpath = paste(dirpath,Sys.Date(),sep="")

 dir.create(dirpath)


 filenames_list <- list.files(path=path, full.names=TRUE, pattern = "*.csv")

 All <- lapply(filenames_list,function(filename){
     print(paste("Merging",filename,sep = " "))
     read.csv(filename)
 })

#Removing unnecessary columns and anonymising the tweets 
 df <- do.call(rbind.data.frame, All)
 df <- df[ -c(7:16, 18:63, 85:90) ]
 
 df1 <- df[5]
 
 write.csv(df1,"./data/merged_text_all.csv")
 write.csv(df,merge_file_name)
 
 All <- lapply(filenames_list,function(filename){
     print(paste("Moving",filename,sep = " "))
     file.move(filename,dirpath)
 })
 


```


```{r, eval=FALSE}

library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(openxlsx)

filename <- ''
filename = paste(filename,"./data/", sep='')
filename = paste(filename,'merged_text_all', sep='')
filename = paste(filename,'.csv',sep='')
Dataset2 <- read.csv(filename)
tweets.df <- Dataset2$text
# convert text to lowercase
tweets.df<-tolower(tweets.df)

# get rid of problem characters
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

# get rid of unnecessary spaces
tweets.df <- str_replace_all(tweets.df," "," ")
# get rid of URLs
tweets.df <- str_replace_all(tweets.df, "https://t.co/[a-z,A-Z,0-9]*","")
# Rake out the retweet header (there is only one)
tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")
# get rid of hashtags
tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")
# get rid of references to other screen names
tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")
# Replace new lines
tweets.df <- str_replace_all(tweets.df, "[\r\n]" , " ")
# remove punctuation, digits, special characters etc
tweets.df = gsub("&amp", " ", tweets.df)
tweets.df= gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweets.df)
tweets.df = gsub("@\\w+", "", tweets.df)
tweets.df <- str_replace_all(tweets.df,"â€™","'")
tweets.df= gsub(".*?($|'|[^[:punct:]]).*?", "\\1",tweets.df)
tweets.df = gsub("[[:digit:]]", "", tweets.df)
tweets.df = gsub("http\\w+", " ", tweets.df)
tweets.df = gsub("[ \t]{2,}", " ", tweets.df)
tweets.df= gsub("^\\s+|\\s+$", "", tweets.df)
tweets.df <- gsub('[0-9]+', "", tweets.df)

View(tweets.df)

write.csv(tweets.df, file="./data/cleanedText.csv",row.names = FALSE)

```


```{r,eval=FALSE}
#Text processing to remove derogatory words and lemmatisation
library(textstem)
cleandf <- read.csv('./data/cleanedText.csv')
rem <- c("*****","*****","*****","*****","*****","*****","*****","*****","*****","*****","*****","*****","*****") #removing tweets containing derogatory words
cleandf <- cleandf[!grepl(paste(rem, collapse="|"), cleandf$x),]
cleandf$text <- gsub('[0-9]+', "", cleandf$x)
cleandf$text <-gsub("^ *|(?<= ) | *$", "", cleandf$text, perl = TRUE)
cleandf$text <-gsub('(?<=\\s)(\\w{1,2}\\s)','',cleandf$text,perl=T)
cleandf[cleandf==""] <- NA
cleandf <- na.omit(cleandf)
cleandf <-cleandf["text"]
cleanch <-unique(cleandf$text)
lemmatised <- unlist(lapply(cleanch,lemmatize_strings))

keepdf<-data.frame("text"=lemmatised)
keep <- c("\\<i\\>","\\<my\\>","\\<our\\>","\\<ours\\>","\\<im\\>","i'm","\\<ive\\>","i've","ourselves","\\<me\\>","i'd","\\<weve\\>","we've","\\<we\\>","\\<us\\>") #keeping tweets containing self words
keepdf<- keepdf[grepl(paste(keep, collapse="|"), keepdf$x),]
write.csv(keepdf,"./data/individual_data.csv",row.names = FALSE)

```



```{r,eval=FALSE}
#Finding top words

library(tibble)
library(dplyr)
library(tidyverse)
library(tokenizers)
library(tidytext)

cleandf = read.csv("./data/individual_data.csv")

cleandf$text <- gsub('[0-9]+', "", cleandf$text)
cleandf$text <-gsub("^ *|(?<= ) | *$", "", cleandf$text, perl = TRUE)
cleandf[cleandf==""] <- NA
cleandf <- na.omit(cleandf)

myStopWords <- tibble(
  word = c(
    "coronavirus", "covid19", "isolation","quarantine","self","test","testing","selfisolation","TheLockdown","StayAtHome","lockdown","socialdistancing","quarantine","SideEffectsOfQuarantineLife","Work","from","home","workfromhome","StayAtHomeChallenge","QuarantineAndChill","StayAtHomeAndStaySafe","SocialDistancing","IStayHome","StayHomeSaveLives","TogetherAtHome","WashYourHands","self-isolation","isolation","social","distance","week","day","time","days","weeks","people","stay","at","home","uk","nhs","covid","house","staff","isolate"),
  lexicon = "twitter"
)

tidy_tweets<- cleandf %>%
    select(text) %>%
    unnest_tokens("word", text)

all_stop_words <- stop_words %>%
  bind_rows(myStopWords)

tidy_rm_tweets<-tidy_tweets %>%
      anti_join(all_stop_words)


top_words <- tidy_rm_tweets %>%
  group_by(word) %>%
  tally %>%
  arrange(desc(n)) %>%
  head(10)

print(top_words)

```


```{r,eval=FALSE}

#Overall Sentiment Analysis

library(plyr)
library(stringr)
library(NLP)
library(ggplot2)
library(tm)
library(scales)

cleandf = read.csv("./data/individual_data.csv")
cleandf$text <- gsub('[0-9]+', "", cleandf$text)
cleandf$text <- gsub("^ *|(?<= ) | *$", "", cleandf$text, perl = TRUE)
cleandf[cleandf==""] <- NA
cleandf <- na.omit(cleandf)

tweets.df <- cleandf$text

#Sentiment analysis POLARITY (-ve, +ve, neutral)

#Reading the Lexicon positive and negative words
pos <- readLines("./data/positive_words.txt")
neg <- readLines("./data/negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
 {
   # Parameters
   # sentences: vector of text to score
   # pos.words: vector of words of postive sentiment
   # neg.words: vector of words of negative sentiment
   # .progress: passed to laply() to control of progress bar
   
   # create simple array of scores with laply
   scores <- laply(sentences,
                   function(sentence, pos.words, neg.words)
                   {
                     # remove punctuation
                     sentence <- gsub("[[:punct:]]", "", sentence)
                     # remove control characters
                     sentence <- gsub("[[:cntrl:]]", "", sentence)
                     # remove digits
                     sentence <- gsub('\\d+', '', sentence)
                     
                     #convert to lower
                     sentence <- tolower(sentence)
                     # split sentence into words with str_split (stringr package)
                     word.list <- str_split(sentence, "\\s+")
                     words <- unlist(word.list)
                     
                     # compare words to the dictionaries of positive & negative terms
                    pos.matches <- match(words, pos)
                    neg.matches <- match(words, neg)

                     # get the position of the matched term or NA
                     # we just want a TRUE/FALSE
                    pos.matches <- !is.na(pos.matches)
                    neg.matches <- !is.na(neg.matches)

                     # final score
                    score <- sum(pos.matches) - sum(neg.matches)
                    return(score)
                  }, pos.words, neg.words, .progress=.progress )
   # data frame with scores for each sentence
  scores.df <- data.frame(text=sentences, score=scores)
  return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')

 #Summary of the sentiment scores
summary(scores_twitter)
scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negative', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
View(scores_twitter)
write.csv(scores_twitter,file='./data/scores_twitter_individual.xlsx')

#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")

#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) + ggtitle("Overall Sentiment Distribution") +
  scale_y_continuous(labels = percent)+labs(y="Score")+
  theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1


```

```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= " Sentiment distribution of tweet corpus", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("./Results/OverallSentiments.png")

```


```{r,eval=FALSE} 
#Emotion Detection Overall
library(syuzhet)
library(plotly)
library(tm)
library(wordcloud)


tweets <- read.csv("./data/individual_data.csv")
clean_tweets <- as.character(tweets$text)

emotions <- get_nrc_sentiment(clean_tweets)
emotions <- emotions[!(names(emotions) %in% c("positive","negative"))] #remove positive and negative sentiments
emo_bar = colSums(emotions)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])

emo_sum <- emo_sum[1:8,]

emo_sum$percent<-(emo_sum$count/sum(emo_sum$count))*100


#Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~percent, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""),  yaxis = list(title = "Emotion count"),
showlegend=FALSE,title="Distribution of emotion categories") %>%
layout(yaxis = list(ticksuffix = "%"))

```


```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= " Emotion categories of the tweet corpus", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("./Results/OverallEmotions.png")

```


```{r,eval=FALSE}
#Topic Modelling to create topic labels

library(LDAvis)
library(tm)
library(lda)
library(shiny)
library(stringr)
library(textstem)
library(plyr)

cleandf = read.csv("./data/individual_data.csv") 
#Removing tweets which are less than 3 words
cleandf <- cleandf[sapply(strsplit(as.character(cleandf$text)," "),length)>3,]
cleandf$text <-gsub("^ *|(?<= ) | *$", "", cleandf$text, perl = TRUE)
#Removing tweets which consists of only empty spaces
cleandf[cleandf==" "] <- NA
cleandf <- na.omit(cleandf)
cleanch <- unique(cleandf$text)
# tokenize on space and output as a list:
doc.list <- strsplit(cleanch, "[[:space:]]+")

#Removing some custom stopwords which are highly densely used in the tweets: Trending words
stopWords <- c(stopwords("SMART"),"coronavirus", "covid", "isolation","quarantine", "social","distance","virus","corona","stay","safe","people","pandemic","selfisolation","lockdown","time","weekend","day","week","good","socialdistancing","uk")

# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stopWords | term.table < 10
term.table <- term.table[!del]
vocab <- names(term.table)

#write.csv(vocab,'./data/vocab.csv')
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}

documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents)  # number of documents
W <- length(vocab)  # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus


# MCMC and model tuning parameters:
K <- 5
G <- 5000
alpha <- 0.02 
eta <- 0.01 

#Fit the model:
library(lda)
set.seed(146)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 5, vocab = vocab,
                                   num.iterations = 500, alpha = 0.02, eta=0.01,
                                    initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)

t2 <- Sys.time()
t2 - t1

#LDAvis
theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))

tweetvis <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)


# create the JSON object to feed the visualization:
json <- createJSON(phi = tweetvis$phi,
                   theta = tweetvis$theta,
                   doc.length = tweetvis$doc.length,
                   vocab = tweetvis$vocab,
                   term.frequency = tweetvis$term.frequency)
serVis(json, out.dir = tempfile(), open.browser = interactive())

cleanText<-data.frame("text"=cleanch)

topicNum <-apply(theta, 1, which.max)
cleanText$topic <-topicNum
count(cleanText$topic)
#write.csv(cleanText,'./data/data_predicted_topics_individual.csv',row.names=FALSE)

topic1 <- subset(cleanText, topic == "1")
write.csv(topic1,'./data/topic1.csv',row.names = FALSE)
topic2 <- subset(cleanText, topic == "2")
write.csv(topic2,'./data/topic2.csv',row.names = FALSE)
topic3 <- subset(cleanText, topic == "3")
write.csv(topic3,'./data/topic3.csv',row.names = FALSE)
topic4 <- subset(cleanText, topic == "4")
write.csv(topic4,'./data/topic4.csv',row.names = FALSE)
topic5 <- subset(cleanText, topic == "5")
write.csv(topic5,'./data/topic5.csv',row.names = FALSE)

```


<!-- ```{r} -->
<!-- #merged this with topic modelling -->

<!-- pred_text <- read.csv('./data/data_predicted_topics_individual.csv') -->

<!-- topic1 <- subset(cleanText, topic == "1") -->
<!-- write.csv(topic1,'./data/topic1_new.csv',row.names = FALSE) -->
<!-- topic2 <- subset(cleanText, topic == "2") -->
<!-- write.csv(topic2,'./data/topic2_new.csv',row.names = FALSE) -->
<!-- topic3 <- subset(cleanText, topic == "3") -->
<!-- write.csv(topic3,'./data/topic3_new.csv',row.names = FALSE) -->
<!-- topic4 <- subset(cleanText, topic == "4") -->
<!-- write.csv(topic4,'./data/topic4_new.csv',row.names = FALSE) -->
<!-- topic5 <- subset(cleanText, topic == "5") -->
<!-- write.csv(topic5,'./data/topic5_new.csv',row.names = FALSE) -->
<!-- ``` -->

```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= "LDAvis Representation for the Topic Models", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("./Results/Topic1.png")
include_graphics("./Results/Topic2.png")
include_graphics("./Results/Topic3.png")
include_graphics("./Results/Topic4.png")
include_graphics("./Results/Topic5.png")

```

```{r,eval=FALSE}

#Sentiment Analysis of each topic

#Reading the Lexicon positive and negative words
pos <- readLines("./data/positive_words.txt")
neg <- readLines("./data/negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
   # Parameters
   # sentences: vector of text to score
   # pos.words: vector of words of postive sentiment
   # neg.words: vector of words of negative sentiment
   # .progress: passed to laply() to control of progress bar

   # create simple array of scores with laply
   scores <- laply(sentences,
                   function(sentence, pos.words, neg.words)
                   {
                     # remove punctuation
                     sentence <- gsub("[[:punct:]]", "", sentence)
                     # remove control characters
                     sentence <- gsub("[[:cntrl:]]", "", sentence)
                     # remove digits
                     sentence <- gsub('\\d+', '', sentence)

                     #convert to lower
                     sentence <- tolower(sentence)
                     # split sentence into words with str_split (stringr package)
                     word.list <- str_split(sentence, "\\s+")
                     words <- unlist(word.list)

                     # compare words to the dictionaries of positive & negative terms
                     pos.matches <- match(words, pos)
                     neg.matches <- match(words, neg)

                     # get the position of the matched term or NA
                     # we just want a TRUE/FALSE
                     pos.matches <- !is.na(pos.matches)
                     neg.matches <- !is.na(neg.matches)

                     # final score
                     score <- sum(pos.matches) - sum(neg.matches)
                     return(score)
                   }, pos.words, neg.words, .progress=.progress )
   # data frame with scores for each sentence
   scores.df <- data.frame(text=sentences, score=scores)
   return(scores.df)
 }
 
 
#Sentiment Analysis for each topic

files <- list.files('./data/')
files <- files[grep("topic", files)]

#create empty list
lst <- vector("list", length(files))

#Read files in to list
for(i in 1:length(files)) {
    path <- './data/'
    path <- paste(path,files[i],sep="")
    lst[[i]] <- read.csv(path)
}

#Apply a function to the list
lst <- lapply(lst, function(x){

  scores_twitter<-score.sentiment(as.character(x$text), pos.txt, neg.txt, .progress='text')
  x$score<-scores_twitter$score
return(x)}
)
#Summary of the sentiment scores
df <- ldply(lst)
tapply(df$score, df$topic, mean)

```


```{r,eval=FALSE} 
#Word cloud creation of each topic : was used for text exploration

library(wordcloud)

topic1 <- read.csv('./data/topic1.csv')
topic2 <- read.csv('./data/topic2.csv')
topic3 <- read.csv('./data/topic3.csv')
topic4 <- read.csv('./data/topic4.csv')
topic5 <- read.csv('./data/topic5.csv')

tweets.df <- as.character(topic1$text) #Manually modified to find the results
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
stopWords <- c(stopwords("smart"),"coronavirus", "covid", "isolation","quarantine","social","distance","virus","corona","stay","safe","people","pandemic","selfisolation","lockdown")
tweet_clean <- tm_map(tweet_clean, removeWords, stopWords)
wordcloud(tweet_clean, random.order=0.3,max.words=100, col=rainbow(50),min.freq = 10,  scale=c(2.0,0.3))

```


```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= "Word cloud topic wise", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("./Results/wordcloud_topic1.png")
include_graphics("./Results/wordcloud_topic2.png")
include_graphics("./Results/wordcloud_topic3.png")
include_graphics("./Results/wordcloud_topic4.png")
include_graphics("./Results/wordcloud_topic5.png")

```

```{r,eval=FALSE} 
#Emotion Detection Topic wise
library(syuzhet)
library(plotly)
library(tm)
library(wordcloud)

topic1 <- read.csv('./data/topic1.csv')
topic2 <- read.csv('./data/topic2.csv')
topic3 <- read.csv('./data/topic3.csv')
topic4 <- read.csv('./data/topic4.csv')
topic5 <- read.csv('./data/topic5.csv')

clean_tweets <- as.character(topic1$text) #Manually modified to find the results

emotions <- get_nrc_sentiment(clean_tweets)
emotions <- emotions[!(names(emotions) %in% c("positive","negative"))] #remove positive and negative sentiments
emo_bar = colSums(emotions)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])

emo_sum <- emo_sum[1:8,]

emo_sum$percent<-(emo_sum$count/sum(emo_sum$count))*100


#Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~percent, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""),  yaxis = list(title = "Emotion count"),
showlegend=FALSE,title="Distribution of emotion categories") %>%
layout(yaxis = list(ticksuffix = "%"))

```


```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= "Emotion distribution topic wise", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("./Results/Emotions_Topic1.png")
include_graphics("./Results/Emotions_Topic2.png")
include_graphics("./Results/Emotions_Topic3.png")
include_graphics("./Results/Emotions_Topic4.png")
include_graphics("./Results/Emotions_Topic5.png")

```
